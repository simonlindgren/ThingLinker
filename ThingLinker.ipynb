{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ThingLinker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python script by Simon Lindgren // [@simonlindgren](http://www.twitter.com/simonlindgren) // [simonlindgren.com](http://simonlindgren.com).\n",
    "\n",
    "[Actor-Network theory](https://en.wikipedia.org/wiki/Actor%E2%80%93network_theory) (ANT) is about how human and non-human actants are connected in relational systems. It sees entities (humans, texts, machines, activitiees, ideas) as linked to each other in heterogeneous networks. Actors appear in any shape or material. The important thing is not if they have human agency, but whether they have the capacity to cause difference in the course of action of other entitites or not. \n",
    "\n",
    "One way of analysing processes like these is to look at mechanisms through which an actor is connected to to other actors, and how those other actors in turn are linked to each other. Such analyses can be starting point of making closer assessments of things such as [obligatory passage points](https://en.wikipedia.org/wiki/Obligatory_passage_point), [interessement](https://en.wikipedia.org/wiki/Interessement), [enrolment, and mobilisation](https://en.wikipedia.org/wiki/Translation_(sociology)). In short, networks are continually made and re-made, by actors who draw links and associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Required Python libraries\n",
    "import glob, re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en\") # Set up spaCy with the the default model for English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing text data\n",
    "The ThingLinker starts from a text dataset that we want to analyse from the perspective of ANT. The data should be in the form of `.txt` files in a `/data` subdirectory to the ThingLinker script. We consider each line in the input as a document, so lines could be units like tweets, blog posts, books, chapters, paragraphs, articles, article sections, etc. – all depending on how we want to calculate the links further on. ThingLinker will analyse how `Things` are `Linked` based on if they co-occur _within_ the documents defined here.\n",
    "\n",
    "We read the data into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs = glob.glob(\"data/*.txt\")                # the files in our data directory\n",
    "dataset = []                                # an empty dataset (Python list)\n",
    "\n",
    "for f in fs:                                # iterate over the files\n",
    "    data = open(f, 'r').readlines()         # read each line in the file\n",
    "    for l in data:                          \n",
    "        dataset.append(l)                   # add it to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then clean the data from things that we do no want. The code below removes urls, any non-alphanumeric characters, double spaces, double line-breaks, and empty lines. Note however, that from the perspective of ANT, things such as urls or emojis can definitely be interesting as actors, so the code below must be customised for the research task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean = []\n",
    "\n",
    "for line in dataset:\n",
    "    line = re.sub(r'(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?$', \" \", line)\n",
    "    line = re.sub('[^0-9a-zA-Z]+', ' ', line)\n",
    "    line = re.sub('  ', ' ', line)\n",
    "    line = re.sub(r'(\\n\\n)','\\n', line)\n",
    "    if not len(line.strip()) == 0 :\n",
    "        clean.append(line)\n",
    "\n",
    "dataset = clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect the\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Things\n",
    "\n",
    "As our next step, we use the [`spaCy`](https://spacy.io) Python library to extract what language technologists call '[Named Entities](https://en.wikipedia.org/wiki/Named-entity_recognition)'. These entities – 'Things' – are considered here as potential actors, in the sense of ANT. With `spaCy`, we will get the following tags:\n",
    "\n",
    "- PERSON\tPeople, including fictional.\n",
    "- NORP\tNationalities or religious or political groups.\n",
    "- FACILITY\tBuildings, airports, highways, bridges, etc.\n",
    "- ORG\tCompanies, agencies, institutions, etc.\n",
    "- GPE\tCountries, cities, states.\n",
    "- LOC\tNon-GPE locations, mountain ranges, bodies of water.\n",
    "- PRODUCT\tObjects, vehicles, foods, etc. (Not services.)\n",
    "- EVENT\tNamed hurricanes, battles, wars, sports events, etc.\n",
    "- WORK_OF_ART\tTitles of books, songs, etc.\n",
    "- LANGUAGE\tAny named language.\n",
    "\n",
    "It also extracts the following values:\n",
    "\n",
    "- DATE\tAbsolute or relative dates or periods.\n",
    "- TIME\tTimes smaller than a day.\n",
    "- PERCENT\tPercentage, including \"%\".\n",
    "- MONEY\tMonetary values, including unit.\n",
    "- QUANTITY\tMeasurements, as of weight or distance.\n",
    "- ORDINAL\t\"first\", \"second\", etc.\n",
    "- CARDINAL\tNumerals that do not fall under another type.\n",
    "\n",
    "In addition to these, the ThingLinker will extract any number of `MANUAL` entities that are defined by the researcher depending on the issue at hand. Such entities could be things that are for some reason not caught by `spaCy` but still interesting to the analysis, or names of emotions, activites or any other thing. These entities to be extracted are defined in the `my_things` list.\n",
    "\n",
    "The code below does the following:\n",
    "* Sets up an empty list called `tagged` and writes a string of column names to it.\n",
    "* Sets up a variable called `itemnumber` to use as a counter for processed items.\n",
    "* Iterates through all the items in our `dataset` and does two things:\n",
    "    - First, it uses the `nlp` model that we imported from `spaCy` to extract Named Entities from the item.\n",
    "    - Second, it checks if any user-defined things (stored in the `my_things` list) occur in the item.\n",
    "    - Third, adds any extracted things to `tagged`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged = []\n",
    "tagged.append(\"doc;type;thing\") #column names\n",
    "\n",
    "my_things = ['love', 'hate']\n",
    "\n",
    "itemnumber = 0\n",
    "\n",
    "for item in dataset:\n",
    "    itemnumber = itemnumber + 1\n",
    "    print(\"Parsing item \" + str(itemnumber) + \" of \" + str(len(dataset)), end='\\r')\n",
    "    parsed = nlp(item.strip())\n",
    "    ents = list(parsed.ents)\n",
    "    sents = list(parsed.sents)\n",
    "    for ent in ents:\n",
    "        entstring = \"\"\n",
    "        entstring += str(itemnumber) + \"; \" + ent.label_ + \"; \" + ent.text\n",
    "        tagged.append(entstring)\n",
    "    for sent in sents:\n",
    "        bag = str(sent).split() # sentence to bag-of-words\n",
    "        for thing in my_things: # look for my things in the bag\n",
    "            if thing in bag:\n",
    "                sentstring = \"\"\n",
    "                sentstring += str(itemnumber) + \"; MANUAL; \" + thing\n",
    "                tagged.append(sentstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write to a csv file\n",
    "with open(\"tagged.csv\",\"w\") as out:\n",
    "    for line in tagged:\n",
    "        out.write(line)\n",
    "        out.write('\\n')\n",
    "\n",
    "# Read the csv into a dataframe        \n",
    "tagged_df = pd.DataFrame.from_csv('tagged.csv', sep = \";\", index_col = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the process above, we extracted data using some of the properties that `spaCy` parsed (namely `.ents`, and `.sents`). We can inspect other available properties of the parsed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linking Things\n",
    "\n",
    "Having extracted the things we want, we now analyse how they are connected witin the analysed documents. To do this, we first filter out the two columns that we need from the tagged dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_filtered = tagged_df[['doc','thing']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we use `pandas.merge()` to get the [Cartesian product](https://www.reddit.com/r/explainlikeimfive/comments/1kznwi/eli5_cartesian_product/) of all the rows which have the same document, i.e. a list of all occurring pairs and in which documents they happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pid = pairs in docs\n",
    "df_pid = pd.merge(df_filtered, df_filtered, on='doc')\n",
    "\n",
    "# Also, remove all pairs where thing_x == thing_y (self-loops)\n",
    "df_pid = df_pid.query(\"thing_x < thing_y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, let's group the dataframe (`.groupby()`) by the pairs and count the number of distinct documents in which the pair appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_grouped = df_pid.groupby(by=['thing_x', 'thing_y']).agg({'doc': 'nunique'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `.reset_index()` to flatten the hierarchical grouping and get weighted pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pairs = df_grouped.reset_index()\n",
    "\n",
    "# Rename the columns into Gephi terminology\n",
    "df_pairs.rename(columns={'thing_x' : 'Source', 'thing_y' : 'Target', 'doc': 'Weight'}, inplace=True)\n",
    "\n",
    "df_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write files for network analysis\n",
    "\n",
    "ThingLinker will output two files – a node table and an edge table – to use in further network analysis using – for example – [Gephi](https://gephi.org/users/supported-graph-formats/spreadsheet/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NODE TABLE\n",
    "\n",
    "# Extract data from the tagged_df dataframe\n",
    "df_nodes = tagged_df[['thing', 'thing', 'type']]\n",
    "\n",
    "# Rename the columns into Gephi terminology\n",
    "df_nodes.columns = ['Id', 'Label', 'Type']\n",
    "\n",
    "# Remove duplicate rows\n",
    "df_nodes = df_nodes.drop_duplicates()\n",
    "\n",
    "# Write the nodetable to csv\n",
    "df_nodes.to_csv('thinglinker_nodes.csv', index=False, header=True, sep=';')\n",
    "\n",
    "print(len(df_nodes), \"nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EDGE TABLE\n",
    "\n",
    "df_pairs.to_csv('thinglinker_edges.csv', index=False, header=True, sep=';')\n",
    "\n",
    "print(len(df_pairs), \"edges\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
